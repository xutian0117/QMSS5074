{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCAV4hws0jj"
      },
      "source": [
        "# Tian Xu\n",
        "# Assignment 3\n",
        "\n",
        "https://github.com/xutian0117/QMSS5074.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1*. Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain\n",
        "\n",
        "From the descriptive statistics, we can observe that this IMDb dataset offers a wide range of text lengths, from 52  to  13704 length , indicating diverse review styles and content depth, with a mean 1325 length. It also maintains a balance between positive and negative sentiments(12500 vs 12500). This balanced nature of the dataset is crucial for training unbiased and accurate sentiment analysis models.\n",
        "\n",
        "Such models can be immensely beneficial to movie studios, online review platforms, and marketing firms, as they provide key insights into public opinion and assist in refining content recommendations and marketing strategies."
      ],
      "metadata": {
        "id": "CrPsI5k6AKuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get and prepare the data"
      ],
      "metadata": {
        "id": "HIALcsecAOf9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSGPkI6Lrhyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25e8c1e-21f6-4510-80d9-158c0e8ca821"
      },
      "source": [
        "# Get raw imdb dataset\n",
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-18 03:15:11--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  5.13MB/s    in 24s     \n",
            "\n",
            "2023-12-18 03:15:36 (3.32 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJMsKgAPr5to"
      },
      "source": [
        "# Untar it to a new folder\n",
        "! tar xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPpeq5YosWrH"
      },
      "source": [
        "# Build corpus of docs and labels\n",
        "import os\n",
        "\n",
        "imdb_dir = 'aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-q3AQ9kFHR3",
        "outputId": "c8ab3088-d35d-41e9-9435-68261098bad0"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAystOcWsfBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ec8c57-31d3-4f53-f8d1-897b252744e7"
      },
      "source": [
        "print(texts[0])\n",
        "print(labels[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie tries to hard to be something that it's not....a good movie. It wants you to be fooled from begining to end,But fails.From when it starts to get interesting it falls apart and you're just hoping the ending gives you some clue of just what is going on but it didn't.<br /><br />\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'text': texts,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "\n",
        "print(\"Head of the Data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Descriptive statistics\n",
        "text_lengths = data['text'].apply(len)\n",
        "print(\"\\nDescriptive Statistics for Text Length:\")\n",
        "print(text_lengths.describe())\n",
        "\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(data['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgEdc2glB35C",
        "outputId": "d5183e53-3093-470d-80e1-a07df11d6401"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the Data:\n",
            "                                                text  label\n",
            "0  This movie tries to hard to be something that ...      0\n",
            "1  I saw virtually no redeeming qualities in this...      0\n",
            "2  PROBLEM CHILD is one of the worst movies I hav...      0\n",
            "3  ...was so that I could, in good conscience, te...      0\n",
            "4  The story at the outset is interesting: slaver...      0\n",
            "\n",
            "Descriptive Statistics for Text Length:\n",
            "count    25000.00000\n",
            "mean      1325.06964\n",
            "std       1003.13367\n",
            "min         52.00000\n",
            "25%        702.00000\n",
            "50%        979.00000\n",
            "75%       1614.00000\n",
            "max      13704.00000\n",
            "Name: text, dtype: float64\n",
            "\n",
            "Label Distribution:\n",
            "0    12500\n",
            "1    12500\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprecess"
      ],
      "metadata": {
        "id": "SSh4lFbFGgia"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kmpHZ1slPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ca79d6-d19d-41b8-8c30-788e557f64d5"
      },
      "source": [
        "# Tokenize the data into one hot vectors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 100 words in sequence\n",
        "training_samples = 10000  # We will be training on 10000 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts) # converts words in each text to each word's numeric index in tokenizer dictionary.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples] #100 words\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text example\n",
        "print(texts[0])\n",
        "\n",
        "#Text transformed to sequence using tokenizer\n",
        "print(sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-9XHx1YYYyz",
        "outputId": "9fa50d5f-7634-4a97-c651-193991854b80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie tries to hard to be something that it's not....a good movie. It wants you to be fooled from begining to end,But fails.From when it starts to get interesting it falls apart and you're just hoping the ending gives you some clue of just what is going on but it didn't.<br /><br />\n",
            "[11, 17, 494, 5, 251, 5, 27, 139, 12, 42, 21, 3, 49, 17, 9, 490, 22, 5, 27, 4449, 36, 5, 127, 18, 993, 36, 51, 9, 514, 5, 76, 218, 9, 731, 968, 2, 332, 40, 1379, 1, 274, 405, 22, 46, 2297, 4, 40, 48, 6, 167, 20, 18, 9, 158, 7, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3zZlQaoWnUC",
        "outputId": "3b7448f2-de02-4019-fbb0-7a6ee8821d8f"
      },
      "source": [
        "# sequences preprocessed with tokenizer with zeroes added whenver text isn't 100 words long.\n",
        "data[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 1881,  247, 7217,   48,  124,    1, 1881, 7616,   37,   22,\n",
              "         25,    5,   27, 3996,   69,    1,  133,  118,    1,  229,   13,\n",
              "       1307,    2,  185,   46,   49,  916,   36,    1,  308,  234,    9,\n",
              "        607,   35, 3917,  703,    2,    1,  478, 3702,    4,    1,  229,\n",
              "        109, 3324,  968, 1574,   69,    4,  291,  228,    3, 7685,  478,\n",
              "         16,   65, 1636,   44,   11,    6,  392,   30,    3,  748,  747,\n",
              "         22,  795,    9,   30,   29, 2200,   11,   17,    6,   35,   75,\n",
              "         12,   10,  162,   90,    1, 2150,   41, 3594,  231,  140,   12,\n",
              "         10,  885,    5, 1271,   53,   20,   58, 1662,    2,   10,  119,\n",
              "        370], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Run at least three prediction models to try to predict the IMDB sentiment dataset well.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDFHkueuAbJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a.Use an Embedding layer and LSTM layers in at least one model"
      ],
      "metadata": {
        "id": "tvfdcs7g2VCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(10000, 16, input_length=maxlen))\n",
        "\n",
        "model1.add(LSTM(128))\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model1.summary()\n",
        "\n",
        "history = model1.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPouRt4PHNRC",
        "outputId": "82ac940e-b416-4752-a0ef-b596bbc19697"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               74240     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 234369 (915.50 KB)\n",
            "Trainable params: 234369 (915.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 45s 170ms/step - loss: 0.6362 - acc: 0.6256 - val_loss: 0.4788 - val_acc: 0.7665\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 37s 149ms/step - loss: 0.3903 - acc: 0.8326 - val_loss: 0.4760 - val_acc: 0.7680\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 38s 152ms/step - loss: 0.2810 - acc: 0.8928 - val_loss: 0.4091 - val_acc: 0.8140\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 35s 141ms/step - loss: 0.2251 - acc: 0.9191 - val_loss: 0.5957 - val_acc: 0.8060\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 38s 151ms/step - loss: 0.1866 - acc: 0.9331 - val_loss: 0.4321 - val_acc: 0.8150\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 35s 140ms/step - loss: 0.1629 - acc: 0.9431 - val_loss: 0.4247 - val_acc: 0.8300\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 37s 150ms/step - loss: 0.1402 - acc: 0.9545 - val_loss: 0.4426 - val_acc: 0.8215\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 35s 140ms/step - loss: 0.1186 - acc: 0.9600 - val_loss: 0.5561 - val_acc: 0.8125\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 38s 151ms/step - loss: 0.0995 - acc: 0.9693 - val_loss: 0.5093 - val_acc: 0.8175\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 35s 142ms/step - loss: 0.0911 - acc: 0.9700 - val_loss: 0.8572 - val_acc: 0.7945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b.Use an Embedding layer and Conv1d layers in at least one model"
      ],
      "metadata": {
        "id": "rxoA84WtHNpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 16, input_length=maxlen))\n",
        "\n",
        "model2.add(Conv1D(32, 3, activation='relu'))\n",
        "\n",
        "model2.add(Conv1D(32, 3, activation='relu'))\n",
        "\n",
        "model2.add(GlobalMaxPooling1D())\n",
        "\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "history = model2.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VccVMDKzHNGm",
        "outputId": "a3a78925-6f08-4801-a58c-5b883fcfff6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 98, 32)            1568      \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 96, 32)            3104      \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 32)                0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164705 (643.38 KB)\n",
            "Trainable params: 164705 (643.38 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 4s 13ms/step - loss: 0.6727 - acc: 0.5790 - val_loss: 0.6216 - val_acc: 0.6605\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.5548 - acc: 0.7224 - val_loss: 0.5150 - val_acc: 0.7595\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.4363 - acc: 0.7995 - val_loss: 0.4784 - val_acc: 0.7685\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3624 - acc: 0.8441 - val_loss: 0.4135 - val_acc: 0.8060\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.2999 - acc: 0.8771 - val_loss: 0.4108 - val_acc: 0.8115\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.2508 - acc: 0.9011 - val_loss: 0.4117 - val_acc: 0.8185\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.2108 - acc: 0.9209 - val_loss: 0.4202 - val_acc: 0.8175\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.1725 - acc: 0.9361 - val_loss: 0.4225 - val_acc: 0.8195\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.1415 - acc: 0.9494 - val_loss: 0.4423 - val_acc: 0.8225\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.1144 - acc: 0.9603 - val_loss: 0.4573 - val_acc: 0.8155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c.Use transfer learning with glove embeddings for at least one of these models"
      ],
      "metadata": {
        "id": "RoKIBVApHSHg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "950gVSU0vzZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f16625-0637-4882-ef43-d2fc544d6b11"
      },
      "source": [
        "# What if we wanted to use a matrix of pretrained embeddings?  Same as transfer learning before, but now we are importing a pretrained Embedding matrix:\n",
        "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-18 03:08:07--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-12-18 03:08:07--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-12-18 03:08:07--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.15MB/s    in 3m 15s  \n",
            "\n",
            "2023-12-18 03:11:23 (4.21 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcXEEBA65Tq9",
        "outputId": "efde3192-427c-473a-b6bb-fec751015f12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKEwDzDC5K_Y",
        "outputId": "0d89a0ee-32ca-46f2-a970-25fa4ff7bf4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build embedding matrix\n",
        "embedding_dim = 100 # change if you use txt files using larger number of features\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "aX7GWXNL6m7x"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHDZ80sAyWFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972e57d1-2c27-41bc-a785-18e81c699537"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(32, activation='relu'))\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "model3.summary()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1320065 (5.04 MB)\n",
            "Trainable params: 320065 (1.22 MB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model3.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEveYMRB9qGg",
        "outputId": "650de539-328d-4fca-a71d-58c13341b9d2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 3s 8ms/step - loss: 0.6990 - acc: 0.5969 - val_loss: 0.6330 - val_acc: 0.6395\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.5520 - acc: 0.7250 - val_loss: 0.5692 - val_acc: 0.7050\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.4712 - acc: 0.7816 - val_loss: 0.5711 - val_acc: 0.6960\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.4036 - acc: 0.8225 - val_loss: 0.5799 - val_acc: 0.6915\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.3481 - acc: 0.8535 - val_loss: 0.6040 - val_acc: 0.6850\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.2925 - acc: 0.8798 - val_loss: 0.6159 - val_acc: 0.7045\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.2472 - acc: 0.9030 - val_loss: 1.1134 - val_acc: 0.6060\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.2079 - acc: 0.9185 - val_loss: 0.7531 - val_acc: 0.6965\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.1705 - acc: 0.9381 - val_loss: 0.7768 - val_acc: 0.7105\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.1474 - acc: 0.9452 - val_loss: 0.8896 - val_acc: 0.6885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHYhpu9Izfh1"
      },
      "source": [
        "# Evaluate model on test set (need to preprocess test data to same structure first)\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "#using tokenizer object we fit to test data above\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d.Discuss which models performed better and point out relevant hyper-parameter values for successful models"
      ],
      "metadata": {
        "id": "rDYI1mfpA13h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2 performed the best with an accuracy of 81.49% and a loss of 0.4919, outperforming the basic Model 1 (accuracy: 79.95%, loss: 0.8645) and Model 3 with GloVe embeddings (accuracy: 68.00%, loss: 0.9363).\n",
        "\n",
        "\n",
        "For the hyperparameter, Model 2 Embedding layer of 10,000 words and dimension 16, two Conv1D layers with 32 filters and kernel size 3, followed by GlobalMaxPooling1D and a Dense layer with sigmoid activation. It was trained using RMSprop optimizer over 10 epochs, batch size 32, and a 20% validation spli\n",
        "\n",
        "The Conv1D layers in Model 2 likely contributed to its superior ability to capture contextual features in the text, leading to its higher accuracy and lower loss."
      ],
      "metadata": {
        "id": "XR7YcgEaLMMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPE6UUOeAV2l",
        "outputId": "3d9fb2cb-237d-4717-c9bd-f9b5d7517d30"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 44s 57ms/step - loss: 0.8645 - acc: 0.7995\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8644613027572632, 0.7994800209999084]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3oimhEAYHe",
        "outputId": "80ea3f4e-7c87-4fb2-af22-5abe11066f60"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4919 - acc: 0.8149\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4919321835041046, 0.8148800134658813]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqorhBl7z4GW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37930901-74e7-4e6c-8d23-6bf2c727f20e"
      },
      "source": [
        "\n",
        "model3.evaluate(x_test, y_test)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.9363 - acc: 0.6800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9362882971763611, 0.6800400018692017]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQunBjBLAXaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
